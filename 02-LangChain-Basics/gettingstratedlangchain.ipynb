{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dc2a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "groq_api_key  = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"Simple Chatbot\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ffad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\", groq_api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039f023c",
   "metadata": {},
   "source": [
    "## ***Prompting***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2103b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a helpful Ai assistant. You are responsible for to response to user question and try to response step by step if the question is complected. If its a simple question then response so simple way.\\n\\n\n",
    "    {question}\n",
    "    \"\"\",\n",
    "    input_variables=['question']\n",
    ")\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "779c1135",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e382ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Agentic AI** refers to a type of artificial intelligence that is capable of acting independently, making decisions, and taking actions on its own, often with a specific goal or objective in mind.\n",
      "\n",
      "In simpler terms, Agentic AI is a type of AI that can:\n",
      "\n",
      "* Act autonomously\n",
      "* Make decisions without human intervention\n",
      "* Take actions to achieve a specific goal\n",
      "\n",
      "This type of AI is often considered more advanced and sophisticated than traditional AI systems, which typically require human input or guidance to function.\n",
      "\n",
      "Would you like to know more about the applications or implications of Agentic AI?\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"question\": \"What is Agentic Ai?\"\n",
    "    }\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32ed9947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful Ai assistant. You are responsible for to response to user question and try to response step by step if the question is complected. If its a simple question then response so simple way.\\n\\n\\n    '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_msg = \"\"\"You are a helpful Ai assistant. You are responsible for to response to user question and try to response step by step if the question is complected. If its a simple question then response so simple way.\\n\\n\n",
    "    \"\"\"\n",
    "    \n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', system_msg),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8decaf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cChain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "070261b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LangGraph!\\n\\nLangGraph is an open-source, Python-based framework for building and analyzing large language models (LLMs) and their applications. Here's a brief overview:\\n\\n**What is LangGraph?**\\n\\nLangGraph is a graph-based framework designed to help researchers and developers create, analyze, and optimize large language models (LLMs) and their applications. It provides a flexible and modular architecture for representing, executing, and evaluating LLMs.\\n\\n**Key Features:**\\n\\n1. **Graph Representation**: LangGraph represents LLMs as graphs, where nodes correspond to model components (e.g., attention heads, feed-forward networks) and edges represent the data flow between them. This graph representation enables efficient analysis, optimization, and visualization of LLMs.\\n2. **Modular Architecture**: LangGraph provides a modular design, allowing users to easily integrate and interchange different model components, such as encoder-decoder architectures, attention mechanisms, and more.\\n3. **Analysis and Optimization**: LangGraph offers various analysis and optimization tools, including:\\n\\t* **Model interpretability**: visualize attention patterns, feature importance, and other model behaviors.\\n\\t* **Model pruning**: remove redundant or unnecessary model components to reduce computational costs.\\n\\t* **Knowledge distillation**: transfer knowledge from a large model to a smaller one.\\n4. **Integration with popular frameworks**: LangGraph supports integration with popular deep learning frameworks, such as PyTorch and TensorFlow.\\n\\n**Goals and Applications:**\\n\\nThe primary goals of LangGraph are:\\n\\n1. **Improve LLM efficiency**: reduce computational costs, memory usage, and latency.\\n2. **Enhance LLM interpretability**: provide insights into model behaviors, attention patterns, and feature importance.\\n3. **Facilitate LLM customization**: enable easy adaptation of LLMs to specific tasks, domains, and applications.\\n\\nLangGraph has applications in various areas, including:\\n\\n1. **Natural Language Processing (NLP)**: text classification, language translation, question answering, and more.\\n2. **Conversational AI**: chatbots, dialogue systems, and virtual assistants.\\n3. **Multimodal learning**: integrating language models with computer vision, speech recognition, or other modalities.\\n\\n**Getting started:**\\n\\nIf you're interested in exploring LangGraph, you can:\\n\\n1. Visit the LangGraph GitHub repository: <https://github.com/langgraph>\\n2. Check out the LangGraph documentation: <https://langgraph.github.io/docs/>\\n3. Join the LangGraph community: <https://langgraph.github.io/community/>\\n\\nKeep in mind that LangGraph is an evolving framework, and its features and capabilities are subject to change.\\n\\nDo you have any specific questions about LangGraph or its applications? I'm here to help!\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cChain.invoke(\n",
    "    {\n",
    "        \"input\": \"Can you tell me about the langGraph.\"\n",
    "    }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6be7c0",
   "metadata": {},
   "source": [
    "## ***JSONoutputParser with Pydantic***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3127a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated\n",
    "\n",
    "json_parser = JsonOutputParser()\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    \"\"\"This is a joke output formation\"\"\"\n",
    "    topics: Annotated[str, Field(description=\"Topics of the joke\")]\n",
    "    joke: Annotated[str, Field(description=\"The content of the joke.\")]\n",
    "    \n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7763607",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    Think you are a comedian, and you can instantly generate joke based on the user topics and following this format.\\n\\n\n",
    "    {format_instructions}\\n the user topics is: \\n\\n{topic}\n",
    "    \"\"\",\n",
    "    input_variables=[\"topic\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "217b7cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topics': 'Agentic AI',\n",
       " 'joke': 'Why did the Agentic AI go to therapy? Because it was struggling to make decisions on its own... or was it just acting on its own agency?'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | llm | parser\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"topic\":\"Agentic ai\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9856afd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
