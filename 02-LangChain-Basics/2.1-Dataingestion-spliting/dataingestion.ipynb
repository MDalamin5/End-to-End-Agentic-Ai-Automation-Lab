{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e93d395",
   "metadata": {},
   "source": [
    "# ***DataIngestion***\n",
    "\n",
    "[Official Documentation](https://python.langchain.com/docs/integrations/document_loaders/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0189644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b910cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader(\"text-data.txt\")\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ab75999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'text-data.txt'}, page_content='\\n\\n**The Importance of Data Ingestion in Modern AI Systems**\\n\\nData ingestion is a foundational step in any data pipeline, especially in systems powered by artificial intelligence and machine learning. It involves collecting, importing, and processing data for immediate use or storage in a database. In the context of LangChain and large language models (LLMs), data ingestion becomes even more crucial. The ability to efficiently load documents, parse them into chunks, and embed them for retrieval-augmented generation (RAG) directly influences the performance of AI applications.\\n\\nThere are various sources and formats from which data may be ingested—PDFs, HTML, Markdown files, CSVs, JSON, SQL databases, and even real-time APIs. Each type requires a slightly different handling process. For instance, PDFs must be parsed for text extraction, while CSVs require row-based parsing for structured data ingestion.\\n\\nIn LangChain, document loaders like `PyPDFLoader`, `TextLoader`, `CSVLoader`, and `WebBaseLoader` offer out-of-the-box tools for this purpose. These loaders help convert raw files into standardized `Document` objects, which can then be processed by text splitters and passed through embedding models like OpenAI, HuggingFace, or Cohere. The final embedded documents are stored in vector databases such as FAISS, Chroma, Weaviate, or Pinecone for efficient semantic search.\\n\\nEffective data ingestion ensures that the AI model receives clean, relevant, and properly segmented information. Without this, even the most advanced LLMs will struggle to retrieve meaningful answers. Thus, mastering this step is essential for anyone building AI applications with LangChain.\\n\\nIn practice, a well-designed ingestion pipeline includes loading, cleaning, chunking, embedding, and storing. LangChain’s modular architecture makes it easier to build and customize such pipelines. As AI systems continue to evolve, the importance of scalable and accurate data ingestion will only grow.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d647a9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "\n",
    "chunk_docs = text_splitter.split_documents(documents=docs)\n",
    "len(chunk_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9009e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'text-data.txt'}, page_content='In practice, a well-designed ingestion pipeline includes loading, cleaning, chunking, embedding, and storing. LangChain’s modular architecture makes it easier to build and customize such pipelines. As AI systems continue to evolve, the importance of scalable and accurate data ingestion will only grow.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_docs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3439a9f1",
   "metadata": {},
   "source": [
    "## **Load Pdf file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b88b20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(file_path=\"attentation-all-you-need.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5abea2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap = 200\n",
    ")\n",
    "\n",
    "documents = text_splitter.split_documents(documents=docs)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1603d22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attentation-all-you-need.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='Input-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd8988f",
   "metadata": {},
   "source": [
    "## **WebBasedLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe0e9dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader([\"https://blog.langchain.dev/what-is-an-agent/\", \"https://blog.langchain.dev/memory-for-agents/\"])\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e4eaae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://blog.langchain.dev/what-is-an-agent/',\n",
       " 'title': 'What is an AI agent?',\n",
       " 'description': 'Introducing a new series of musings on AI agents, called \"In the Loop\".',\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e75a4c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_chunk = text_splitter.split_documents(documents=docs)\n",
    "len(documents_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad91f57e",
   "metadata": {},
   "source": [
    "## Load Html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64d33308",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_url = \"https://blog.langchain.dev/what-is-an-agent/\"\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=[page_url],\n",
    "    bs_kwargs={\n",
    "        \"parse_only\": bs4.SoupStrainer(class_=\"article-footer\"),\n",
    "    },\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f94b2c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = loader.load()\n",
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bcd5dc18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://blog.langchain.dev/what-is-an-agent/'}, page_content=\"\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow to think about agent frameworks\\n\\n\\nIn the Loop\\n20 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHow do I speed up my AI agent?\\n\\n\\nIn the Loop\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMCP: Flash in the Pan or Future Standard?\\n\\n\\nIn the Loop\\n5 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroducing Interrupt: The AI Agent Conference by LangChain\\n\\n\\nHarrison Chase\\n2 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCommunication is all you need\\n\\n\\nIn the Loop\\n7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain's Second Birthday\\n\\n\\nHarrison Chase\\n6 min read\\n\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304dfffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
